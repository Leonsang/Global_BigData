# ‚ö° SPARK & PYSPARK ESSENTIALS CHEATSHEET
## Lo que S√ç vas a usar todos los d√≠as (80% del tiempo)

---

## üöÄ **INICIALIZACI√ìN B√ÅSICA**

### **Setup Spark Session**
```python
from pyspark.sql import SparkSession

# Crear Spark Session (SIEMPRE primero)
spark = SparkSession.builder \
    .appName("MiProyecto") \
    .master("spark://spark-master:7077") \
    .config("spark.executor.memory", "2g") \
    .config("spark.executor.cores", "2") \
    .getOrCreate()

# Para uso local sin cluster
spark = SparkSession.builder \
    .appName("Local") \
    .master("local[*]") \
    .getOrCreate()
```

### **Verificar que Todo Funciona**
```python
# Test b√°sico
spark.version
spark.sparkContext.getConf().getAll()

# Test con datos
df = spark.range(1000)
df.count()  # Deber√≠a devolver 1000
```

---

## üìÅ **LECTURA DE DATOS**

### **Desde HDFS**
```python
# CSV desde HDFS
df = spark.read \
    .option("header", "true") \
    .option("inferSchema", "true") \
    .csv("hdfs://namenode:9000/mi_proyecto/raw/datos.csv")

# Parquet (formato optimizado)
df = spark.read.parquet("hdfs://namenode:9000/mi_proyecto/parquet/")

# JSON
df = spark.read.json("hdfs://namenode:9000/mi_proyecto/json/")
```

### **Desde Archivos Locales**
```python
# CSV local
df = spark.read \
    .option("header", "true") \
    .option("inferSchema", "true") \
    .csv("file:///opt/spark-data/datos.csv")

# M√∫ltiples archivos
df = spark.read \
    .option("header", "true") \
    .csv("hdfs://namenode:9000/mi_proyecto/raw/*.csv")
```

### **Desde Bases de Datos**
```python
# PostgreSQL/MySQL
df = spark.read \
    .format("jdbc") \
    .option("url", "jdbc:postgresql://localhost:5432/midb") \
    .option("dbtable", "transacciones") \
    .option("user", "usuario") \
    .option("password", "password") \
    .load()
```

---

## üîç **EXPLORACI√ìN B√ÅSICA**

### **Informaci√≥n General**
```python
# Ver estructura
df.printSchema()
df.columns
df.count()
df.describe().show()

# Primeras filas
df.show(10)
df.head(5)

# Tipos de datos
df.dtypes
```

### **Estad√≠sticas R√°pidas**
```python
# Resumen estad√≠stico
df.describe("columna_numerica").show()

# Valores √∫nicos
df.select("categoria").distinct().count()

# Valores nulos
df.filter(df.columna.isNull()).count()

# Distribuci√≥n de valores
df.groupBy("categoria").count().show()
```

---

## üõ†Ô∏è **TRANSFORMACIONES ESENCIALES**

### **Seleccionar y Filtrar**
```python
# Seleccionar columnas
df.select("columna1", "columna2").show()

# Filtrar filas
df.filter(df.edad > 25).show()
df.filter("edad > 25 AND salario < 50000").show()

# Renombrar columnas
df.withColumnRenamed("old_name", "new_name")

# Agregar nueva columna
from pyspark.sql.functions import col, lit
df.withColumn("nueva_col", col("col1") + col("col2"))
df.withColumn("constante", lit("valor_fijo"))
```

### **Agregaciones B√°sicas**
```python
from pyspark.sql.functions import sum, avg, max, min, count

# Group by simple
df.groupBy("categoria") \
  .agg(sum("ventas").alias("total_ventas"),
       avg("precio").alias("precio_promedio")) \
  .show()

# M√∫ltiples agregaciones
df.groupBy("region", "categoria") \
  .agg(count("*").alias("total_registros"),
       max("fecha").alias("ultima_fecha")) \
  .show()
```

### **Joins**
```python
# Inner join (m√°s com√∫n)
resultado = df1.join(df2, df1.id == df2.customer_id, "inner")

# Left join
resultado = df1.join(df2, "id", "left")

# Join con m√∫ltiples condiciones
resultado = df1.join(df2, 
    (df1.id == df2.customer_id) & (df1.fecha == df2.fecha))
```

---

## üïê **FUNCIONES DE TIEMPO**

```python
from pyspark.sql.functions import year, month, dayofweek, date_format

# Extraer componentes de fecha
df.withColumn("a√±o", year("fecha")) \
  .withColumn("mes", month("fecha")) \
  .withColumn("dia_semana", dayofweek("fecha")) \
  .show()

# Formatear fechas
df.withColumn("fecha_formato", 
    date_format("timestamp", "yyyy-MM-dd HH:mm:ss")).show()
```

---

## üíæ **GUARDAR RESULTADOS**

### **A HDFS**
```python
# Parquet (recomendado para Big Data)
df.write \
  .mode("overwrite") \
  .parquet("hdfs://namenode:9000/mi_proyecto/procesados/resultado")

# CSV
df.coalesce(1) \
  .write \
  .mode("overwrite") \
  .option("header", "true") \
  .csv("hdfs://namenode:9000/mi_proyecto/resultados/")

# Particionado por columna (para archivos grandes)
df.write \
  .mode("overwrite") \
  .partitionBy("a√±o", "mes") \
  .parquet("hdfs://namenode:9000/mi_proyecto/particionado/")
```

---

## üéØ **SQL EN SPARK**

```python
# Crear vista temporal
df.createOrReplaceTempView("ventas")

# Ejecutar SQL
resultado = spark.sql("""
    SELECT categoria,
           SUM(monto) as total_ventas,
           AVG(precio) as precio_promedio
    FROM ventas 
    WHERE fecha >= '2024-01-01'
    GROUP BY categoria
    ORDER BY total_ventas DESC
""")

resultado.show()
```

---

## ‚ö° **OPTIMIZACI√ìN PERFORMANCE**

### **Caching**
```python
# Cache para reutilizar DataFrame
df.cache()
df.persist()

# Verificar cache
spark.catalog.isCached("nombre_tabla")

# Limpiar cache
df.unpersist()
spark.catalog.clearCache()
```

### **Partitioning**
```python
# Repartir para balance
df.repartition(10)

# Repartir por columna (para joins)
df.repartition("id")

# Coalescer para reducir archivos
df.coalesce(1)
```

### **Broadcast para Joins**
```python
from pyspark.sql.functions import broadcast

# Para tablas peque√±as (<200MB)
resultado = df_grande.join(
    broadcast(df_peque√±o), "id"
)
```

---

## üêõ **DEBUGGING & TROUBLESHOOTING**

### **Ver Plan de Ejecuci√≥n**
```python
# Ver qu√© va a hacer Spark
df.explain()
df.explain(True)  # M√°s detallado

# Ejecutar y ver m√©tricas
df.show()
print(f"Particiones: {df.rdd.getNumPartitions()}")
```

### **Sampling para Tests**
```python
# Muestra peque√±a para testing
df_sample = df.sample(0.1)  # 10% de los datos
df_small = df.limit(1000)   # Primeros 1000 registros
```

### **Manejo de Errores**
```python
try:
    resultado = df.collect()
except Exception as e:
    print(f"Error: {e}")
    
# Ver errores en logs
spark.sparkContext.setLogLevel("WARN")
```

---

## üìä **WINDOW FUNCTIONS**

```python
from pyspark.sql.window import Window
from pyspark.sql.functions import row_number, rank, lag

# Ranking dentro de grupos
window = Window.partitionBy("categoria").orderBy(col("ventas").desc())

df.withColumn("ranking", row_number().over(window)) \
  .show()

# Comparaci√≥n con valor anterior
window_tiempo = Window.partitionBy("producto").orderBy("fecha")
df.withColumn("ventas_anterior", lag("ventas").over(window_tiempo)) \
  .show()
```

---

## üîÑ **PATRONES COMUNES DE ETL**

### **Limpieza de Datos**
```python
# Eliminar nulls
df_clean = df.filter(df.columna.isNotNull())

# Reemplazar valores
df_clean = df.fillna({"columna_str": "Sin datos", "columna_num": 0})

# Eliminar duplicados
df_unique = df.dropDuplicates(["id"])
```

### **Transformaci√≥n de Columnas**
```python
from pyspark.sql.functions import when, regexp_replace, upper

# Condicionales
df.withColumn("categoria_nueva", 
    when(col("precio") > 100, "Premium")
    .when(col("precio") > 50, "Medio")
    .otherwise("B√°sico")
).show()

# Limpiar texto
df.withColumn("nombre_limpio", 
    regexp_replace(upper(col("nombre")), "[^A-Z0-9]", "")
).show()
```

---

## üé™ **EJEMPLO COMPLETO DE SESI√ìN**

```python
# 1. Inicializar
from pyspark.sql import SparkSession
from pyspark.sql.functions import *

spark = SparkSession.builder \
    .appName("AnalisisTransporte") \
    .getOrCreate()

# 2. Cargar datos
df = spark.read \
    .option("header", "true") \
    .option("inferSchema", "true") \
    .csv("hdfs://namenode:9000/transporte/raw/viajes.csv")

# 3. Explorar
print(f"Total registros: {df.count()}")
df.printSchema()
df.show(5)

# 4. Limpiar
df_clean = df.filter(df.distancia > 0) \
            .filter(df.precio.isNotNull())

# 5. Analizar
resultado = df_clean.groupBy("tipo_vehiculo") \
    .agg(
        avg("distancia").alias("distancia_promedio"),
        sum("precio").alias("ingresos_totales"),
        count("*").alias("total_viajes")
    ) \
    .orderBy(desc("ingresos_totales"))

# 6. Mostrar resultados
resultado.show()

# 7. Guardar
resultado.write \
    .mode("overwrite") \
    .parquet("hdfs://namenode:9000/transporte/procesados/analisis_vehiculos")

# 8. Limpiar
spark.stop()
```

---

## üö® **ERRORES COMUNES Y SOLUCIONES**

### **Error: "Master URL not set"**
```python
# ‚ùå Falta master
spark = SparkSession.builder.appName("test").getOrCreate()

# ‚úÖ Con master
spark = SparkSession.builder \
    .appName("test") \
    .master("spark://spark-master:7077") \
    .getOrCreate()
```

### **Error: "File not found"**
```python
# ‚ùå Path incorrecto
df = spark.read.csv("/mi_archivo.csv")

# ‚úÖ Path completo HDFS
df = spark.read.csv("hdfs://namenode:9000/mi_proyecto/mi_archivo.csv")
```

### **Error: "Out of memory"**
```python
# ‚ùå Cache de todo
df.cache()
df.collect()  # Trae todo a driver

# ‚úÖ Solo cache lo necesario + sample
df.sample(0.1).cache()
df.show(20)  # No usar collect()
```

### **Performance Lento**
```python
# ‚ùå Sin partitioning
df.groupBy("categoria").count().show()

# ‚úÖ Con repartitioning
df.repartition("categoria") \
  .groupBy("categoria") \
  .count() \
  .show()
```

---

## üéØ **CHECKLIST DE COMPETENCIAS**

### **B√°sico** ‚úÖ
- [ ] Puedo iniciar Spark Session
- [ ] Leo archivos CSV y Parquet
- [ ] Uso select, filter, groupBy
- [ ] Guardo resultados en HDFS

### **Intermedio** ‚úÖ
- [ ] Optimizo con cache y repartition
- [ ] Uso joins entre DataFrames
- [ ] Aplico window functions
- [ ] Manejo errores comunes

### **Avanzado** ‚úÖ
- [ ] Entiendo explain() plans
- [ ] Resuelvo data skew
- [ ] Uso broadcast joins
- [ ] Optimizo para production

**üí° Tip Final: Siempre inicia con muestra peque√±a (sample/limit), luego escala a datos completos!**
