{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üóÇÔ∏è SESI√ìN 1: HDFS AN√ÅLISIS DE CLUSTER\n",
    "## An√°lisis Distribuido de Datos de Transporte\n",
    "\n",
    "### üéØ **OBJETIVO**\n",
    "Configurar y analizar un cluster HDFS real con datos de transporte de Lima\n",
    "\n",
    "### üìä **DATASET**\n",
    "- 30 d√≠as de datos hist√≥ricos de transporte\n",
    "- +100,000 viajes\n",
    "- 50 rutas, 15 distritos\n",
    "- Particionado por fecha y distrito"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# üîß CONFIGURACI√ìN INICIAL\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from hdfs import InsecureClient\n",
    "\n",
    "# Conectar a HDFS\n",
    "hdfs_client = InsecureClient('http://namenode:9870')\n",
    "\n",
    "# Configurar visualizaciones\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"üîó Conexi√≥n HDFS establecida\")\n",
    "print(f\"üìÅ Directorio ra√≠z: {hdfs_client.list('/')}\")\n",
    "\n",
    "print(\"‚úÖ Configuraci√≥n completada\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä EJERCICIO 1: An√°lisis de Distribuci√≥n HDFS\n",
    "\n",
    "**Problema Real**: Los datos no est√°n distribuidos uniformemente, causando hotspots de performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# 1.1 An√°lisis del Estado del Cluster\n",
    "print(\"üè• AN√ÅLISIS DE SALUD DEL CLUSTER\")\n",
    "print(\"===========================\")\n",
    "\n",
    "try:\n",
    "    # Ejecutar reporte del cluster\n",
    "    result = subprocess.run(['hdfs', 'dfsadmin', '-report'], \n",
    "                          capture_output=True, text=True, timeout=30)\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        print(\"üìã REPORTE DEL CLUSTER:\")\n",
    "        print(result.stdout)\n",
    "        \n",
    "        # Extraer m√©tricas clave\n",
    "        output = result.stdout\n",
    "        metrics = {}\n",
    "        \n",
    "        # Parsear informaci√≥n b√°sica\n",
    "        if 'Live datanodes' in output:\n",
    "            live_nodes = output.split('Live datanodes')[1].split('Dead datanodes')[0]\n",
    "            metrics['live_nodes'] = live_nodes.strip()\n",
    "            print(f\"‚úÖ Nodos activos: {metrics['live_nodes']}\")\n",
    "        \n",
    "        if 'DFS Used' in output:\n",
    "            dfs_used = output.split('DFS Used:')[1].split('\\n')[0].strip()\n",
    "            metrics['dfs_used'] = dfs_used\n",
    "            print(f\"üìä Uso de almacenamiento: {metrics['dfs_used']}\")\n",
    "            \n",
    "    else:\n",
    "        print(f\"‚ùå Error obteniendo reporte: {result.stderr}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Error ejecutando comando: {e}\")\n",
    "    print(\"Continuando con an√°lisis alternativo...\")\n",
    "\n",
    "print(\"\\n‚úÖ An√°lisis de cluster completado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# 1.2 Explorar Datos de Transporte en HDFS\n",
    "print(\"üöå EXPLORACI√ìN DE DATOS DE TRANSPORTE\")\n",
    "print(\"================================\")\n",
    "\n",
    "try:\n",
    "    # Listar estructura de datos\n",
    "    transport_dirs = hdfs_client.list('/transport_data/historical')\n",
    "    print(f\"üìÖ D√≠as de datos disponibles: {len(transport_dirs)}\")\n",
    "    print(f\"üìÅ Primeros 5 directorios: {transport_dirs[:5]}\")\n",
    "    \n",
    "    # Analizar estructura de un d√≠a\n",
    "    if transport_dirs:\n",
    "        sample_day = transport_dirs[0]\n",
    "        day_path = f'/transport_data/historical/{sample_day}'\n",
    "        \n",
    "        districts = hdfs_client.list(day_path)\n",
    "        print(f\"\\nüèôÔ∏è Distritos en {sample_day}: {len(districts)}\")\n",
    "        print(f\"üìç Distritos: {districts[:10]}\")\n",
    "        \n",
    "        # An√°lisis de distribuci√≥n de datos\n",
    "        district_sizes = {}\n",
    "        for district in districts[:5]:\n",
    "            try:\n",
    "                district_path = f'{day_path}/{district}'\n",
    "                files = hdfs_client.list(district_path)\n",
    "                \n",
    "                total_size = 0\n",
    "                for file in files:\n",
    "                    file_path = f'{district_path}/{file}'\n",
    "                    file_info = hdfs_client.status(file_path)\n",
    "                    total_size += file_info['length']\n",
    "                \n",
    "                district_sizes[district] = total_size\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Error analizando {district}: {e}\")\n",
    "        \n",
    "        # Visualizar distribuci√≥n\n",
    "        if district_sizes:\n",
    "            print(f\"\\nüìä DISTRIBUCI√ìN DE DATOS POR DISTRITO:\")\n",
    "            for district, size in sorted(district_sizes.items(), key=lambda x: x[1], reverse=True):\n",
    "                size_mb = size / (1024*1024)\n",
    "                print(f\"   {district}: {size_mb:.2f} MB\")\n",
    "            \n",
    "            # An√°lisis de balance\n",
    "            sizes = list(district_sizes.values())\n",
    "            max_size = max(sizes)\n",
    "            min_size = min(sizes)\n",
    "            ratio = max_size / min_size if min_size > 0 else 0\n",
    "            \n",
    "            if ratio > 2.0:\n",
    "                print(f\"\\n‚ö†Ô∏è DESEQUILIBRIO DETECTADO!\")\n",
    "                print(f\"   Ratio m√°ximo/m√≠nimo: {ratio:.2f}\")\n",
    "                print(f\"   Recomendaci√≥n: Reparticionado necesario\")\n",
    "            else:\n",
    "                print(f\"\\n‚úÖ Distribuci√≥n balanceada (ratio: {ratio:.2f})\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error explorando datos: {e}\")\n",
    "    print(\"Generando datos de ejemplo...\")\n",
    "    \n",
    "    # Generar datos de ejemplo\n",
    "    sample_data = {\n",
    "        'Lima_Centro': 45.2,\n",
    "        'Miraflores': 38.7,\n",
    "        'San_Isidro': 42.1,\n",
    "        'Surco': 15.3,\n",
    "        'Barranco': 8.9\n",
    "    }\n",
    "    \n",
    "    print(\"üìä DATOS DE EJEMPLO (MB por distrito):\")\n",
    "    for district, size in sample_data.items():\n",
    "        print(f\"   {district}: {size} MB\")\n",
    "\n",
    "print(\"\\n‚úÖ Exploraci√≥n de datos completada\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üî• EJERCICIO 2: Simulaci√≥n de Fallos\n",
    "\n",
    "**Problema Real**: Un DataNode se cae durante horario pico. ¬øQu√© pasa con nuestros datos?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# 2.1 Estado Inicial del Cluster\n",
    "print(\"üîç ESTADO INICIAL DEL CLUSTER\")\n",
    "print(\"===========================\")\n",
    "\n",
    "def get_cluster_status():\n",
    "    \"\"\"Obtener estado actual del cluster\"\"\"\n",
    "    try:\n",
    "        result = subprocess.run(['hdfs', 'dfsadmin', '-report'], \n",
    "                              capture_output=True, text=True, timeout=15)\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            output = result.stdout\n",
    "            status = {}\n",
    "            \n",
    "            # Extraer informaci√≥n clave\n",
    "            if 'Live datanodes' in output:\n",
    "                lines = output.split('\\n')\n",
    "                for i, line in enumerate(lines):\n",
    "                    if 'Live datanodes' in line:\n",
    "                        parts = line.split()\n",
    "                        if len(parts) >= 3:\n",
    "                            status['live_nodes'] = int(parts[2].rstrip(':'))\n",
    "                        break\n",
    "            \n",
    "            if 'DFS Used:' in output:\n",
    "                for line in output.split('\\n'):\n",
    "                    if 'DFS Used:' in line:\n",
    "                        status['dfs_used'] = line.split('DFS Used:')[1].strip().split()[0]\n",
    "                        break\n",
    "            \n",
    "            return status\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error obteniendo status: {e}\")\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Obtener estado inicial\n",
    "initial_status = get_cluster_status()\n",
    "\n",
    "if initial_status:\n",
    "    print(\"üìä ESTADO INICIAL:\")\n",
    "    for key, value in initial_status.items():\n",
    "        print(f\"   {key}: {value}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No se pudo obtener estado del cluster\")\n",
    "    print(\"Simulando estado inicial...\")\n",
    "    initial_status = {'live_nodes': 2, 'dfs_used': '1.2GB'}\n",
    "    \n",
    "print(\"\\n‚úÖ Estado inicial registrado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# 2.2 Simular fallo de DataNode\n",
    "print(\"üí• SIMULANDO FALLO DE DATANODE\")\n",
    "print(\"===========================\")\n",
    "\n",
    "try:\n",
    "    # Simular fallo (en un entorno real, esto ser√≠a un comando para detener un DataNode)\n",
    "    print(\"‚ö†Ô∏è Simulando fallo de DataNode...\")\n",
    "    \n",
    "    # Esperar y verificar estado\n",
    "    import time\n",
    "    time.sleep(5)  # Simular tiempo de espera\n",
    "    \n",
    "    # Obtener nuevo estado\n",
    "    new_status = get_cluster_status()\n",
    "    \n",
    "    if new_status:\n",
    "        print(\"\\nüìä ESTADO DESPU√âS DEL FALLO:\")\n",
    "        for key, value in new_status.items():\n",
    "            print(f\"   {key}: {value}\")\n",
    "        \n",
    "        # Analizar impacto\n",
    "        if 'live_nodes' in initial_status and 'live_nodes' in new_status:\n",
    "            nodes_diff = initial_status['live_nodes'] - new_status['live_nodes']\n",
    "            if nodes_diff > 0:\n",
    "                print(f\"\\n‚ö†Ô∏è {nodes_diff} DataNode(s) ca√≠do(s)\")\n",
    "                print(\"‚úÖ HDFS est√° replicando datos autom√°ticamente\")\n",
    "            else:\n",
    "                print(\"\\n‚úÖ Todos los DataNodes est√°n activos\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No se pudo obtener estado despu√©s del fallo\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error en simulaci√≥n: {e}\")\n",
    "\n",
    "print(\"\\n‚úÖ Simulaci√≥n completada\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìù LIMPIEZA FINAL\n",
    "\n",
    "Limpiaremos los recursos utilizados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Limpiar recursos\n",
    "print(\"üßπ LIMPIANDO RECURSOS\")\n",
    "print(\"===================\")\n",
    "\n",
    "try:\n",
    "    # Cerrar conexiones\n",
    "    plt.close('all')\n",
    "    print(\"‚úÖ Recursos de visualizaci√≥n liberados\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error en limpieza: {e}\")\n",
    "\n",
    "print(\"\\n‚úÖ Limpieza completada\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
} 