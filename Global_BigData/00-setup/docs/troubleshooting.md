# ğŸ”§ TROUBLESHOOTING GUIDE
## Soluciones a Problemas Comunes - Big Data Environment

### ğŸ¯ **FILOSOFÃA DE TROUBLESHOOTING**
Como ingeniera de datos experta, siempre sigo este proceso:
1. **Reproducir** el problema
2. **Aislar** la causa raÃ­z  
3. **Verificar** la soluciÃ³n
4. **Documentar** para futuros casos

---

## ğŸ³ **PROBLEMAS DOCKER**

### **Error: "Docker not running"**
```bash
# SÃ­ntomas
Cannot connect to the Docker daemon at unix:///var/run/docker.sock

# SoluciÃ³n Windows
1. Abrir Docker Desktop
2. Esperar que inicie completamente
3. Verificar: docker version

# SoluciÃ³n Linux/Mac
sudo systemctl start docker
# o
sudo service docker start
```

### **Error: "Container fails to start"**
```bash
# DiagnÃ³stico
docker-compose logs [nombre_servicio]

# SoluciÃ³n comÃºn: Conflicto de puertos
netstat -ano | findstr :9870
# Si puerto ocupado, cambiar en docker-compose.yml

# SoluciÃ³n: Limpiar containers corruptos
docker-compose down
docker system prune -f
docker-compose up -d
```

### **Error: "Out of disk space"**
```bash
# Verificar espacio
docker system df

# Limpiar imÃ¡genes no usadas
docker system prune -a

# Limpiar volÃºmenes
docker volume prune
```

---

## ğŸ—‚ï¸ **PROBLEMAS HDFS**

### **Error: "NameNode not accessible"**
```bash
# SÃ­ntomas
http://localhost:9870 no responde

# DiagnÃ³stico
docker logs hdfs_namenode

# Soluciones comunes
1. Reiniciar NameNode:
   docker-compose restart namenode

2. Si persiste, reset completo:
   docker-compose down -v
   docker-compose up -d

3. Verificar logs detallados:
   docker exec hdfs_namenode cat /hadoop/logs/hadoop-root-namenode-*.log
```

### **Error: "DataNode disconnected"**
```bash
# SÃ­ntomas
hdfs dfsadmin -report muestra nodos muertos

# DiagnÃ³stico  
docker logs hdfs_datanode1
docker logs hdfs_datanode2

# SoluciÃ³n
docker-compose restart datanode1 datanode2

# Verificar reconexiÃ³n
hdfs dfsadmin -report
```

### **Error: "Permission denied in HDFS"**
```bash
# SÃ­ntomas
hdfs dfs -put: Permission denied

# SoluciÃ³n rÃ¡pida (desarrollo)
hdfs dfs -chmod 777 /
hdfs dfs -mkdir /mi_directorio
hdfs dfs -chmod 755 /mi_directorio

# SoluciÃ³n robusta
hdfs dfs -chown $USER:$USER /mi_directorio
```

### **Error: "SafeMode enabled"**
```bash
# SÃ­ntomas
Name node is in safe mode

# Verificar estado
hdfs dfsadmin -safemode get

# Salir del safe mode (SOLO desarrollo)
hdfs dfsadmin -safemode leave

# Si persiste, verificar replicaciÃ³n
hdfs fsck / -files -blocks
```

---

## âš¡ **PROBLEMAS SPARK**

### **Error: "Spark Master not accessible"**
```bash
# SÃ­ntomas
http://localhost:8080 no responde

# DiagnÃ³stico
docker logs spark_master

# SoluciÃ³n
docker-compose restart spark-master

# Verificar configuraciÃ³n
docker exec spark_master cat /spark/conf/spark-env.sh
```

### **Error: "Workers not connecting"**
```bash
# SÃ­ntomas
Spark UI muestra 0 workers

# DiagnÃ³stico
docker logs spark_worker_1
docker logs spark_worker_2

# SoluciÃ³n comÃºn: Network issues
docker-compose down
docker network prune
docker-compose up -d

# Verificar conectividad
docker exec spark_worker_1 ping spark-master
```

### **Error: "OutOfMemoryError en Spark"**
```bash
# SÃ­ntomas
Job fails with java.lang.OutOfMemoryError

# SoluciÃ³n 1: Ajustar configuraciÃ³n en docker-compose.yml
SPARK_WORKER_MEMORY=4g
SPARK_EXECUTOR_MEMORY=2g

# SoluciÃ³n 2: En cÃ³digo PySpark
spark.conf.set("spark.executor.memory", "2g")
spark.conf.set("spark.driver.memory", "1g")

# SoluciÃ³n 3: Reducir tamaÃ±o de datos
df.sample(0.1).write.parquet("sample_data")
```

### **Error: "Spark job muy lento"**
```bash
# DiagnÃ³stico en Spark UI
http://localhost:4040

# Optimizaciones comunes
1. Aumentar paralelismo:
   df.repartition(8)

2. Usar cache inteligentemente:
   df.cache()

3. Evitar shuffles innecesarios:
   df.coalesce(4) en lugar de repartition()

4. Optimizar joins:
   broadcast(df_pequeÃ±o).join(df_grande)
```

---

## ğŸ““ **PROBLEMAS JUPYTER**

### **Error: "Jupyter not accessible"**
```bash
# SÃ­ntomas
http://localhost:8888 no responde

# DiagnÃ³stico
docker logs jupyter_bigdata

# SoluciÃ³n comÃºn
docker-compose restart jupyter

# Si pide token
docker logs jupyter_bigdata | grep token
# Copiar URL completa con token
```

### **Error: "Kernel dies en notebooks"**
```bash
# SÃ­ntomas
Kernel keeps dying cuando ejecutas celdas

# SoluciÃ³n 1: Reiniciar kernel
Kernel -> Restart

# SoluciÃ³n 2: Aumentar memoria del container
En docker-compose.yml:
deploy:
  resources:
    limits:
      memory: 4G

# SoluciÃ³n 3: Limpiar outputs
Cell -> All Output -> Clear
```

### **Error: "No module named 'pyspark'"**
```bash
# Verificar instalaciÃ³n
docker exec jupyter_bigdata pip list | grep pyspark

# Si no estÃ¡ instalado
docker exec jupyter_bigdata pip install pyspark

# O reconstruir imagen
docker-compose build jupyter
docker-compose up -d jupyter
```

---

## ğŸŒ **PROBLEMAS DE RED**

### **Error: "Containers can't communicate"**
```bash
# DiagnÃ³stico
docker network ls
docker network inspect bigdata_bigdata

# SoluciÃ³n: Recrear network
docker-compose down
docker network prune
docker-compose up -d

# Verificar comunicaciÃ³n
docker exec namenode ping datanode1
docker exec spark_master ping spark_worker_1
```

### **Error: "Port already in use"**
```bash
# SÃ­ntomas
Port 9870 is already allocated

# Encontrar proceso
netstat -ano | findstr :9870
# Windows: taskkill /PID <pid> /F
# Linux: kill -9 <pid>

# O cambiar puerto en docker-compose.yml
ports:
  - "9871:9870"  # Usar puerto diferente
```

---

## ğŸ’¾ **PROBLEMAS DE DATOS**

### **Error: "No such file or directory en HDFS"**
```bash
# Verificar path exacto
hdfs dfs -ls /
hdfs dfs -ls /transport_data

# Crear directorios faltantes
hdfs dfs -mkdir -p /transport_data/historical

# Subir datos de nuevo
hdfs dfs -put datos.csv /transport_data/
```

### **Error: "Corrupted blocks in HDFS"**
```bash
# DiagnÃ³stico
hdfs fsck / -files -blocks -locations

# Ver bloques corruptos
hdfs fsck / -list-corruptfileblocks

# SoluciÃ³n (desarrollo)
hdfs dfsadmin -report
# Si replicaciÃ³n baja, aumentar:
hdfs dfs -setrep 2 /archivo_problema
```

---

## ğŸ”„ **RESET COMPLETO DEL AMBIENTE**

### **Cuando nada mÃ¡s funciona**
```bash
# Script automÃ¡tico
./00-setup/scripts/setup_bigdata_env.sh reset

# O manualmente:
cd 03-environments/development

# Parar todo
docker-compose down -v

# Limpiar completamente
docker system prune -a -f
docker volume prune -f

# Reiniciar
docker-compose up -d

# Esperar y validar
sleep 60
python ../../00-setup/scripts/validate_environment.py
```

---

## ğŸ“Š **VALIDACIÃ“N PASO A PASO**

### **Checklist de DiagnÃ³stico**
```bash
# 1. Docker funcionando
docker version
docker-compose version

# 2. Servicios corriendo
docker-compose ps

# 3. Puertos accesibles
curl http://localhost:9870  # HDFS
curl http://localhost:8080  # Spark
curl http://localhost:8888  # Jupyter

# 4. HDFS operativo
hdfs dfs -ls /

# 5. Spark conectado
docker exec spark_master cat /spark/conf/spark-env.sh

# 6. Jupyter funcionando
docker logs jupyter_bigdata | tail -20
```

---

## ğŸ’¡ **TIPS DE EXPERTA**

### **Desarrollo Eficiente**
1. **Siempre verifica logs** antes de buscar en Google
2. **Usa docker-compose logs -f** para debug en tiempo real
3. **Prueba con datos pequeÃ±os** antes de datasets grandes
4. **Documenta cada problema** que resuelvas

### **Performance**
1. **Monitorea recursos** con `docker stats`
2. **Usa cache** inteligentemente en Spark
3. **Particiona datos** por fecha/regiÃ³n en HDFS
4. **Ajusta paralelismo** segÃºn tu hardware

### **Debugging**
1. **Reproduce en ambiente limpio** antes de reportar bugs
2. **AÃ­sla el problema** (Â¿Docker? Â¿HDFS? Â¿Spark? Â¿CÃ³digo?)
3. **Verifica logs** en orden cronolÃ³gico
4. **Compara con configuraciÃ³n funcional**

---

## ğŸ†˜ **ÃšLTIMA INSTANCIA**

Si despuÃ©s de todo esto sigues teniendo problemas:

1. **Documenta exactamente**:
   - QuÃ© comando ejecutaste
   - QuÃ© error obtuviste (screenshot)
   - QuÃ© logs relevantes tienes
   - QuÃ© ya intentaste

2. **Verifica el estado**:
   ```bash
   python 00-setup/scripts/validate_environment.py > debug_report.txt
   ```

3. **Reset y reintÃ©ntalo**:
   ```bash
   ./00-setup/scripts/setup_bigdata_env.sh reset
   ./00-setup/scripts/setup_bigdata_env.sh setup
   ```

**Â¡La mayorÃ­a de problemas se resuelven con reset + setup!** ğŸ¯