# 游냡 Docker Compose - Simulaci칩n Big Data Realista
# Optimizado para aprendizaje con herramientas de producci칩n

version: '3.8'

services:
  # ===== HADOOP ECOSYSTEM =====
  namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    container_name: hdfs_namenode
    restart: unless-stopped
    ports:
      - "9870:9870"  # Web UI
      - "9000:9000"  # HDFS API
    volumes:
      - namenode_data:/hadoop/dfs/name
      - ./02-datasets:/datasets
      - ./06-solutions:/solutions
    environment:
      - CLUSTER_NAME=bigdata_learning_cluster
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
      - HDFS_CONF_dfs_nameservices=cluster1
      - HDFS_CONF_dfs_blocksize=134217728  # 128MB blocks
      - HDFS_CONF_dfs_replication=2        # 2 replicas for learning    env_file:
      - ./hadoop.env
    networks:
      - bigdata_network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9870"]
      interval: 30s
      timeout: 10s
      retries: 3

  # HDFS DataNode 1
  datanode1:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: hdfs_datanode1
    restart: unless-stopped
    volumes:
      - datanode1_data:/hadoop/dfs/data
      - ./02-datasets:/datasets
    environment:
      SERVICE_PRECONDITION: "namenode:9870"
      CORE_CONF_fs_defaultFS: hdfs://namenode:9000
    env_file:
      - ./hadoop.env
    depends_on:
      namenode:
        condition: service_healthy
    networks:
      - bigdata_network

  # HDFS DataNode 2 (Para replicaci칩n real)
  datanode2:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: hdfs_datanode2
    restart: unless-stopped
    volumes:
      - datanode2_data:/hadoop/dfs/data
      - ./02-datasets:/datasets
    environment:
      SERVICE_PRECONDITION: "namenode:9870"
      CORE_CONF_fs_defaultFS: hdfs://namenode:9000
    env_file:
      - ./hadoop.env
    depends_on:
      namenode:
        condition: service_healthy
    networks:
      - bigdata_network

  # ===== SPARK CLUSTER =====
  spark-master:
    image: bde2020/spark-master:3.1.1-hadoop3.2
    container_name: spark_master
    restart: unless-stopped
    ports:
      - "8080:8080"  # Spark Master Web UI
      - "7077:7077"  # Spark Master
    environment:
      - INIT_DAEMON_STEP=setup_spark
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
      - SPARK_CONF_spark_sql_adaptive_enabled=true
      - SPARK_CONF_spark_sql_adaptive_coalescePartitions_enabled=true    volumes:
      - ./02-datasets:/datasets
      - ./01-sessions:/sessions
    depends_on:
      namenode:
        condition: service_healthy
    networks:
      - bigdata_network

  # Spark Worker 1
  spark-worker-1:
    image: bde2020/spark-worker:3.1.1-hadoop3.2
    container_name: spark_worker_1
    restart: unless-stopped
    ports:
      - "8081:8081"
    environment:
      - SPARK_MASTER=spark://spark-master:7077
      - SPARK_WORKER_CORES=2
      - SPARK_WORKER_MEMORY=2g
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
    volumes:
      - ./02-datasets:/datasets
    depends_on:
      - spark-master
    networks:
      - bigdata_network

  # Spark Worker 2
  spark-worker-2:
    image: bde2020/spark-worker:3.1.1-hadoop3.2
    container_name: spark_worker_2
    restart: unless-stopped
    ports:
      - "8082:8082"
    environment:
      - SPARK_MASTER=spark://spark-master:7077
      - SPARK_WORKER_CORES=2
      - SPARK_WORKER_MEMORY=2g
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
    volumes:
      - ./02-datasets:/datasets
    depends_on:
      - spark-master
    networks:
      - bigdata_network

  # ===== JUPYTER LAB (Ambiente de Trabajo) =====
  jupyter:
    image: jupyter/pyspark-notebook:spark-3.4.0
    container_name: jupyter_bigdata
    restart: unless-stopped
    ports:
      - "8888:8888"
    environment:
      - JUPYTER_ENABLE_LAB=yes
      - JUPYTER_TOKEN=""  # Sin contrase침a para desarrollo
      - SPARK_MASTER=spark://spark-master:7077
      - HADOOP_CONF_DIR=/etc/hadoop/conf    volumes:
      - ./01-sessions:/home/jovyan/sessions
      - ./02-datasets:/home/jovyan/datasets  
      - ./04-cheatsheets:/home/jovyan/cheatsheets
      - ./06-solutions:/home/jovyan/solutions
    depends_on:
      - spark-master
      - namenode
    networks:
      - bigdata_network

  # ===== GENERADOR DE DATOS (Simula datos reales) =====
  data-generator:
    build: ./02-datasets/generators
    container_name: transport_data_generator
    restart: unless-stopped
    environment:
      - HDFS_NAMENODE=hdfs://namenode:9000
      - GENERATION_INTERVAL=30  # Datos cada 30 segundos
      - TOTAL_ROUTES=50
      - DISTRICTS=15
    volumes:
      - ./02-datasets:/app/datasets
    depends_on:
      namenode:
        condition: service_healthy
    networks:
      - bigdata_network

  # ===== MONITOREO (Opcional pero realista) =====
  portainer:
    image: portainer/portainer-ce:latest
    container_name: portainer_monitor
    restart: unless-stopped
    ports:
      - "9443:9443"
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - portainer_data:/data
    networks:
      - bigdata_network

# ===== VOLUMES =====
volumes:
  namenode_data:
    driver: local
  datanode1_data:
    driver: local  
  datanode2_data:
    driver: local
  portainer_data:
    driver: local

# ===== NETWORK =====
networks:
  bigdata_network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.20.0.0/16