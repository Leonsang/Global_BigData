#  Docker Compose - Stack Completo 7 Sesiones
# Ambiente Big Data completo: HDFS + Spark + Kafka + Monitoring

version: '3.8'

services:
  # ===== ZOOKEEPER (Required for Kafka) =====
  zookeeper:
    image: confluentinc/cp-zookeeper:7.4.0
    container_name: zookeeper
    restart: unless-stopped
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
      ZOOKEEPER_LOG4J_ROOT_LOGLEVEL: WARN
    networks:
      - bigdata_network
    healthcheck:
      test: ["CMD", "bash", "-c", "echo 'ruok' | nc localhost 2181"]
      interval: 10s
      timeout: 5s
      retries: 5

  # ===== KAFKA CLUSTER =====
  kafka-1:
    image: confluentinc/cp-kafka:7.4.0
    container_name: kafka_broker_1
    restart: unless-stopped
    ports:
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka-1:29092,PLAINTEXT_HOST://localhost:9092
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 2
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 2
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 2
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
      KAFKA_LOG4J_ROOT_LOGLEVEL: WARN
    depends_on:
      zookeeper:
        condition: service_healthy
    networks:
      - bigdata_network
    healthcheck:
      test: ["CMD", "kafka-broker-api-versions", "--bootstrap-server", "localhost:9092"]
      interval: 10s
      timeout: 5s
      retries: 5

  kafka-2:
    image: confluentinc/cp-kafka:7.4.0
    container_name: kafka_broker_2
    restart: unless-stopped
    ports:
      - "9093:9093"
    environment:
      KAFKA_BROKER_ID: 2
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka-2:29093,PLAINTEXT_HOST://localhost:9093
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 2
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 2
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 2
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
      KAFKA_LOG4J_ROOT_LOGLEVEL: WARN
    depends_on:
      zookeeper:
        condition: service_healthy
    networks:
      - bigdata_network
  # ===== HADOOP HDFS CLUSTER =====
  namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    container_name: hdfs_namenode
    restart: unless-stopped
    ports:
      - "9870:9870"  # Web UI
      - "9000:9000"  # HDFS API
    volumes:
      - namenode_data:/hadoop/dfs/name
      - ./../../02-datasets:/datasets
      - ./../../06-solutions:/solutions
    environment:
      - CLUSTER_NAME=bigdata_learning_cluster
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
      - HDFS_CONF_dfs_blocksize=134217728
      - HDFS_CONF_dfs_replication=2
    env_file:
      - ./hadoop.env
    networks:
      - bigdata_network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9870"]
      interval: 30s
      timeout: 10s
      retries: 3

  datanode1:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: hdfs_datanode1
    restart: unless-stopped
    volumes:
      - datanode1_data:/hadoop/dfs/data
      - ./../../02-datasets:/datasets
    environment:
      SERVICE_PRECONDITION: "namenode:9870"
      CORE_CONF_fs_defaultFS: hdfs://namenode:9000
    env_file:
      - ./hadoop.env
    depends_on:
      namenode:
        condition: service_healthy
    networks:
      - bigdata_network

  datanode2:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: hdfs_datanode2
    restart: unless-stopped
    volumes:
      - datanode2_data:/hadoop/dfs/data
      - ./../../02-datasets:/datasets
    environment:
      SERVICE_PRECONDITION: "namenode:9870"
      CORE_CONF_fs_defaultFS: hdfs://namenode:9000
    env_file:
      - ./hadoop.env
    depends_on:
      namenode:
        condition: service_healthy
    networks:
      - bigdata_network

  # ===== SPARK CLUSTER =====
  spark-master:
    image: bde2020/spark-master:3.1.1-hadoop3.2
    container_name: spark_master
    restart: unless-stopped
    ports:
      - "8080:8080"  # Spark Master Web UI
      - "7077:7077"  # Spark Master
    environment:
      - INIT_DAEMON_STEP=setup_spark
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
      - SPARK_CONF_spark_sql_adaptive_enabled=true
      - SPARK_CONF_spark_sql_adaptive_coalescePartitions_enabled=true    volumes:
      - ./../../02-datasets:/datasets
      - ./../../01-sessions:/sessions
    depends_on:
      namenode:
        condition: service_healthy
    networks:
      - bigdata_network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080"]
      interval: 30s
      timeout: 10s
      retries: 3

  spark-worker-1:
    image: bde2020/spark-worker:3.1.1-hadoop3.2
    container_name: spark_worker_1
    restart: unless-stopped
    ports:
      - "8081:8081"
    environment:
      - SPARK_MASTER=spark://spark-master:7077
      - SPARK_WORKER_CORES=2
      - SPARK_WORKER_MEMORY=2g
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
    volumes:
      - ./../../02-datasets:/datasets
    depends_on:
      spark-master:
        condition: service_healthy
    networks:
      - bigdata_network

  spark-worker-2:
    image: bde2020/spark-worker:3.1.1-hadoop3.2
    container_name: spark_worker_2
    restart: unless-stopped
    ports:
      - "8082:8082"
    environment:
      - SPARK_MASTER=spark://spark-master:7077
      - SPARK_WORKER_CORES=2
      - SPARK_WORKER_MEMORY=2g
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
    volumes:
      - ./../../02-datasets:/datasets
    depends_on:
      spark-master:
        condition: service_healthy
    networks:
      - bigdata_network

  # ===== JUPYTER LAB (Desarrollo) =====
  jupyter:
    image: jupyter/pyspark-notebook:spark-3.4.0
    container_name: jupyter_bigdata
    restart: unless-stopped
    ports:
      - "8888:8888"
    environment:
      - JUPYTER_ENABLE_LAB=yes
      - JUPYTER_TOKEN=""  # Sin contrase帽a para desarrollo
      - SPARK_MASTER=spark://spark-master:7077
      - HADOOP_CONF_DIR=/etc/hadoop/conf
    volumes:
      - ./../../01-sessions:/home/jovyan/sessions
      - ./../../02-datasets:/home/jovyan/datasets  
      - ./../../04-cheatsheets:/home/jovyan/cheatsheets
      - ./../../06-solutions:/home/jovyan/solutions
    depends_on:
      - spark-master
      - namenode
      - kafka-1
    networks:
      - bigdata_network
  # ===== MONITORING STACK =====
  prometheus:
    image: prom/prometheus:latest
    container_name: prometheus_monitor
    restart: unless-stopped
    ports:
      - "9090:9090"
    volumes:
      - ./../../05-monitoring/prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--storage.tsdb.retention.time=200h'
      - '--web.enable-lifecycle'
    networks:
      - bigdata_network

  grafana:
    image: grafana/grafana:latest
    container_name: grafana_dashboard
    restart: unless-stopped
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin123
      - GF_USERS_ALLOW_SIGN_UP=false
    volumes:
      - grafana_data:/var/lib/grafana
      - ./../../05-monitoring/grafana:/etc/grafana/provisioning
    depends_on:
      - prometheus
    networks:
      - bigdata_network

  # ===== DATA GENERATOR (Para todas las sesiones) =====
  data-generator:
    build: ./../../02-datasets/generators
    container_name: transport_data_generator
    restart: unless-stopped
    environment:
      - HDFS_NAMENODE=hdfs://namenode:9000
      - KAFKA_BOOTSTRAP_SERVERS=kafka-1:29092,kafka-2:29093
      - GENERATION_MODE=both  # historical + streaming
      - GENERATION_INTERVAL=30
      - HISTORICAL_DAYS=90
      - TOTAL_ROUTES=50
      - DISTRICTS=15
    volumes:
      - ./../../02-datasets:/app/datasets
    depends_on:
      namenode:
        condition: service_healthy
      kafka-1:
        condition: service_healthy
    networks:
      - bigdata_network

  # ===== KAFKA UI (Para monitoreo) =====
  kafka-ui:
    image: provectuslabs/kafka-ui:latest
    container_name: kafka_ui
    restart: unless-stopped
    ports:
      - "8090:8080"
    environment:
      KAFKA_CLUSTERS_0_NAME: bigdata_cluster
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka-1:29092,kafka-2:29093
      KAFKA_CLUSTERS_0_ZOOKEEPER: zookeeper:2181
    depends_on:
      - kafka-1
      - kafka-2
    networks:
      - bigdata_network

  # ===== PORTAINER (Container Management) =====
  portainer:
    image: portainer/portainer-ce:latest
    container_name: portainer_manager
    restart: unless-stopped
    ports:
      - "9443:9443"
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - portainer_data:/data
    networks:
      - bigdata_network

# ===== VOLUMES =====
volumes:
  namenode_data:
    driver: local
  datanode1_data:
    driver: local  
  datanode2_data:
    driver: local
  prometheus_data:
    driver: local
  grafana_data:
    driver: local
  portainer_data:
    driver: local

# ===== NETWORK =====
networks:
  bigdata_network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.20.0.0/16  # ===== HADOOP HDFS CLUSTER =====
  namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    container_name: hdfs_namenode
    restart: unless-stopped
    ports:
      - "9870:9870"  # Web UI
      - "9000:9000"  # HDFS API
    volumes:
      - namenode_data:/hadoop/dfs/name
      - ./../../02-datasets:/datasets
      - ./../../06-solutions:/solutions
    environment:
      - CLUSTER_NAME=bigdata_learning_cluster
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
      - HDFS_CONF_dfs_blocksize=134217728
      - HDFS_CONF_dfs_replication=2
    env_file:
      - ./hadoop.env
    networks:
      - bigdata_network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9870"]
      interval: 30s
      timeout: 10s
      retries: 3

  datanode1:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: hdfs_datanode1
    restart: unless-stopped
    volumes:
      - datanode1_data:/hadoop/dfs/data
      - ./../../02-datasets:/datasets
    environment:
      SERVICE_PRECONDITION: "namenode:9870"
      CORE_CONF_fs_defaultFS: hdfs://namenode:9000
    env_file:
      - ./hadoop.env
    depends_on:
      namenode:
        condition: service_healthy
    networks:
      - bigdata_network

  datanode2:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: hdfs_datanode2
    restart: unless-stopped
    volumes:
      - datanode2_data:/hadoop/dfs/data
      - ./../../02-datasets:/datasets
    environment:
      SERVICE_PRECONDITION: "namenode:9870"
      CORE_CONF_fs_defaultFS: hdfs://namenode:9000
    env_file:
      - ./hadoop.env
    depends_on:
      namenode:
        condition: service_healthy
    networks:
      - bigdata_network

  # ===== SPARK CLUSTER =====
  spark-master:
    image: bde2020/spark-master:3.1.1-hadoop3.2
    container_name: spark_master
    restart: unless-stopped
    ports:
      - "8080:8080"  # Spark Master Web UI
      - "7077:7077"  # Spark Master
    environment:
      - INIT_DAEMON_STEP=setup_spark
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
      - SPARK_CONF_spark_sql_adaptive_enabled=true
      - SPARK_CONF_spark_sql_adaptive_coalescePartitions_enabled=true
    volumes:
      - ./../../02-datasets:/datasets
      - ./../../01-sessions:/sessions
    depends_on:
      namenode:
        condition: service_healthy
    networks:
      - bigdata_network

  spark-worker-1:
    image: bde2020/spark-worker:3.1.1-hadoop3.2
    container_name: spark_worker_1
    restart: unless-stopped
    ports:
      - "8081:8081"
    environment:
      - SPARK_MASTER=spark://spark-master:7077
      - SPARK_WORKER_CORES=2
      - SPARK_WORKER_MEMORY=2g
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
    volumes:
      - ./../../02-datasets:/datasets
    depends_on:
      - spark-master
    networks:
      - bigdata_network

  spark-worker-2:
    image: bde2020/spark-worker:3.1.1-hadoop3.2
    container_name: spark_worker_2
    restart: unless-stopped
    ports:
      - "8082:8082"
    environment:
      - SPARK_MASTER=spark://spark-master:7077
      - SPARK_WORKER_CORES=2
      - SPARK_WORKER_MEMORY=2g
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
    volumes:
      - ./../../02-datasets:/datasets
    depends_on:
      - spark-master
    networks:
      - bigdata_network
  # ===== JUPYTER LAB (Ambiente Principal de Trabajo) =====
  jupyter:
    image: jupyter/pyspark-notebook:spark-3.4.0
    container_name: jupyter_bigdata
    restart: unless-stopped
    ports:
      - "8888:8888"
    environment:
      - JUPYTER_ENABLE_LAB=yes
      - JUPYTER_TOKEN=""  # Sin contrase帽a para desarrollo
      - SPARK_MASTER=spark://spark-master:7077
      - HADOOP_CONF_DIR=/etc/hadoop/conf
      - KAFKA_BOOTSTRAP_SERVERS=kafka-1:29092,kafka-2:29093
    volumes:
      - ./../../01-sessions:/home/jovyan/sessions
      - ./../../02-datasets:/home/jovyan/datasets  
      - ./../../04-cheatsheets:/home/jovyan/cheatsheets
      - ./../../06-solutions:/home/jovyan/solutions
    depends_on:
      - spark-master
      - namenode
      kafka-1:
        condition: service_healthy
    networks:
      - bigdata_network
    user: root
    command: start-notebook.sh --NotebookApp.token='' --NotebookApp.password='' --allow-root

  # ===== DATA GENERATORS =====
  # Generador de datos hist贸ricos
  data-generator-batch:
    build: ./../../02-datasets/generators
    container_name: transport_data_generator_batch
    restart: "no"  # Solo ejecutar una vez
    environment:
      - HDFS_NAMENODE=hdfs://namenode:9000
      - GENERATION_MODE=historical
      - HISTORICAL_DAYS=90
      - TOTAL_ROUTES=50
      - DISTRICTS=15
    volumes:
      - ./../../02-datasets:/app/datasets
    depends_on:
      namenode:
        condition: service_healthy
    networks:
      - bigdata_network

  # Generador de datos streaming (para Sesi贸n 4)
  data-generator-streaming:
    build: ./../../02-datasets/generators
    container_name: transport_data_generator_streaming
    restart: unless-stopped
    environment:
      - HDFS_NAMENODE=hdfs://namenode:9000
      - KAFKA_BOOTSTRAP_SERVERS=kafka-1:29092,kafka-2:29093
      - GENERATION_MODE=streaming
      - GENERATION_INTERVAL=10
      - TOTAL_ROUTES=50
      - DISTRICTS=15
    volumes:
      - ./../../02-datasets:/app/datasets
    depends_on:
      namenode:
        condition: service_healthy
      kafka-1:
        condition: service_healthy
    networks:
      - bigdata_network
    profiles:
      - streaming  # Solo activar para Sesi贸n 4+

  # ===== MONITORING & MANAGEMENT =====
  # Kafka UI para gesti贸n de topics
  kafka-ui:
    image: provectuslabs/kafka-ui:latest
    container_name: kafka_ui
    restart: unless-stopped
    ports:
      - "8090:8080"
    environment:
      KAFKA_CLUSTERS_0_NAME: bigdata-cluster
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka-1:29092,kafka-2:29093
      KAFKA_CLUSTERS_0_ZOOKEEPER: zookeeper:2181
    depends_on:
      kafka-1:
        condition: service_healthy
    networks:
      - bigdata_network
    profiles:
      - streaming
  # Grafana para dashboards de monitoreo
  grafana:
    image: grafana/grafana:latest
    container_name: grafana_monitoring
    restart: unless-stopped
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=bigdata123
      - GF_USERS_ALLOW_SIGN_UP=false
    volumes:
      - grafana_data:/var/lib/grafana
      - ./../../05-monitoring/grafana:/etc/grafana/provisioning
    networks:
      - bigdata_network
    profiles:
      - monitoring

  # Prometheus para m茅tricas
  prometheus:
    image: prom/prometheus:latest
    container_name: prometheus_metrics
    restart: unless-stopped
    ports:
      - "9090:9090"
    volumes:
      - ./../../05-monitoring/prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus_data:/prometheus
    networks:
      - bigdata_network
    profiles:
      - monitoring

  # Portainer para gesti贸n de containers
  portainer:
    image: portainer/portainer-ce:latest
    container_name: portainer_manager
    restart: unless-stopped
    ports:
      - "9443:9443"
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - portainer_data:/data
    networks:
      - bigdata_network

# ===== VOLUMES =====
volumes:
  namenode_data:
    driver: local
  datanode1_data:
    driver: local  
  datanode2_data:
    driver: local
  grafana_data:
    driver: local
  prometheus_data:
    driver: local
  portainer_data:
    driver: local

# ===== NETWORKS =====
networks:
  bigdata_network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.20.0.0/16

# ===== PROFILES EXPLANATION =====
# Para usar perfiles espec铆ficos:
# 
# Sesiones 0-3 (HDFS + Spark):
# docker-compose up -d
#
# Sesi贸n 4+ (+ Kafka Streaming):
# docker-compose --profile streaming up -d
#
# Con Monitoring completo:
# docker-compose --profile streaming --profile monitoring up -d