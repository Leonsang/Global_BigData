{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üóÇÔ∏è SESI√ìN 1: HDFS OPERACIONES AVANZADAS\n",
    "## An√°lisis y Optimizaci√≥n de HDFS\n",
    "\n",
    "### üéØ **OBJETIVO**\n",
    "Aprender operaciones avanzadas de HDFS y t√©cnicas de optimizaci√≥n\n",
    "\n",
    "### üìã **CONTENIDO**\n",
    "- An√°lisis de bloques y replicaci√≥n\n",
    "- Optimizaci√≥n de almacenamiento\n",
    "- Monitoreo y recuperaci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# üîß CONFIGURACI√ìN INICIAL\n",
    "import subprocess\n",
    "import os\n",
    "import json\n",
    "from hdfs import InsecureClient\n",
    "from datetime import datetime\n",
    "\n",
    "# Conectar a HDFS\n",
    "hdfs_client = InsecureClient('http://namenode:9870')\n",
    "\n",
    "# Crear directorio para an√°lisis\n",
    "analysis_dir = '/analisis_avanzado'\n",
    "hdfs_client.makedirs(analysis_dir)\n",
    "\n",
    "print(\"üîó Conexi√≥n HDFS establecida\")\n",
    "print(f\"üìÅ Directorio de an√°lisis: {analysis_dir}\")\n",
    "\n",
    "print(\"‚úÖ Configuraci√≥n completada\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìù EJERCICIO 1: An√°lisis de Bloques\n",
    "\n",
    "Analizaremos la distribuci√≥n de bloques y replicaci√≥n en HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# 1.1 Crear archivo de prueba grande\n",
    "print(\"üìÑ CREANDO ARCHIVO DE PRUEBA\")\n",
    "print(\"=========================\")\n",
    "\n",
    "try:\n",
    "    # Crear archivo local grande\n",
    "    test_file = 'test_large.txt'\n",
    "    with open(test_file, 'w') as f:\n",
    "        for i in range(1000000):  # 1 mill√≥n de l√≠neas\n",
    "            f.write(f\"L√≠nea de prueba {i}\\n\")\n",
    "    \n",
    "    # Subir a HDFS\n",
    "    hdfs_path = f'{analysis_dir}/test_large.txt'\n",
    "    hdfs_client.upload(hdfs_path, test_file)\n",
    "    print(f\"‚úÖ Archivo subido a {hdfs_path}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "\n",
    "print(\"\\n‚úÖ Archivo creado y subido\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# 1.2 Analizar distribuci√≥n de bloques\n",
    "print(\"üìä ANALIZANDO DISTRIBUCI√ìN DE BLOQUES\")\n",
    "print(\"===============================\")\n",
    "\n",
    "try:\n",
    "    # Obtener informaci√≥n de bloques\n",
    "    result = subprocess.run(['hdfs', 'fsck', hdfs_path, '-files', '-blocks', '-locations'], \n",
    "                          capture_output=True, text=True)\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        print(\"Informaci√≥n de bloques:\")\n",
    "        print(result.stdout)\n",
    "        \n",
    "        # Extraer estad√≠sticas\n",
    "        lines = result.stdout.split('\\n')\n",
    "        for line in lines:\n",
    "            if 'Total blocks' in line:\n",
    "                print(f\"\\nüìä {line}\")\n",
    "            elif 'Average block size' in line:\n",
    "                print(f\"üìä {line}\")\n",
    "    else:\n",
    "        print(f\"‚ùå Error: {result.stderr}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "\n",
    "print(\"\\n‚úÖ An√°lisis completado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìù EJERCICIO 2: Optimizaci√≥n de Almacenamiento\n",
    "\n",
    "Implementaremos t√©cnicas de optimizaci√≥n de almacenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# 2.1 Configurar factor de replicaci√≥n\n",
    "print(\"‚öôÔ∏è CONFIGURANDO FACTOR DE REPLICACI√ìN\")\n",
    "print(\"================================\")\n",
    "\n",
    "try:\n",
    "    # Cambiar factor de replicaci√≥n\n",
    "    result = subprocess.run(['hdfs', 'dfs', '-setrep', '-w', '2', hdfs_path], \n",
    "                          capture_output=True, text=True)\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        print(\"‚úÖ Factor de replicaci√≥n actualizado\")\n",
    "        print(result.stdout)\n",
    "    else:\n",
    "        print(f\"‚ùå Error: {result.stderr}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "\n",
    "print(\"\\n‚úÖ Configuraci√≥n completada\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# 2.2 Implementar compresi√≥n\n",
    "print(\"üóúÔ∏è IMPLEMENTANDO COMPRESI√ìN\")\n",
    "print(\"========================\")\n",
    "\n",
    "try:\n",
    "    # Crear archivo comprimido\n",
    "    compressed_path = f'{analysis_dir}/test_compressed.gz'\n",
    "    \n",
    "    # Comprimir archivo\n",
    "    result = subprocess.run(['hdfs', 'dfs', '-copyFromLocal', '-f', test_file, compressed_path], \n",
    "                          capture_output=True, text=True)\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        # Verificar tama√±os\n",
    "        original_size = hdfs_client.status(hdfs_path)['length']\n",
    "        compressed_size = hdfs_client.status(compressed_path)['length']\n",
    "        \n",
    "        print(f\"üìä Tama√±o original: {original_size} bytes\")\n",
    "        print(f\"üìä Tama√±o comprimido: {compressed_size} bytes\")\n",
    "        print(f\"üìä Ratio de compresi√≥n: {original_size/compressed_size:.2f}x\")\n",
    "    else:\n",
    "        print(f\"‚ùå Error: {result.stderr}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "\n",
    "print(\"\\n‚úÖ Compresi√≥n completada\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìù EJERCICIO 3: Monitoreo y Recuperaci√≥n\n",
    "\n",
    "Implementaremos t√©cnicas de monitoreo y recuperaci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# 3.1 Monitorear estado del cluster\n",
    "print(\"üìä MONITOREANDO ESTADO DEL CLUSTER\")\n",
    "print(\"=============================\")\n",
    "\n",
    "try:\n",
    "    # Obtener m√©tricas del cluster\n",
    "    result = subprocess.run(['hdfs', 'dfsadmin', '-report'], \n",
    "                          capture_output=True, text=True)\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        # Extraer m√©tricas importantes\n",
    "        lines = result.stdout.split('\\n')\n",
    "        metrics = {}\n",
    "        \n",
    "        for line in lines:\n",
    "            if 'Live datanodes' in line:\n",
    "                metrics['datanodes'] = line.split(':')[1].strip()\n",
    "            elif 'Dead datanodes' in line:\n",
    "                metrics['dead_nodes'] = line.split(':')[1].strip()\n",
    "            elif 'Configured Capacity' in line:\n",
    "                metrics['capacity'] = line.split(':')[1].strip()\n",
    "            elif 'DFS Used' in line:\n",
    "                metrics['used'] = line.split(':')[1].strip()\n",
    "        \n",
    "        print(\"üìä M√©tricas del cluster:\")\n",
    "        for key, value in metrics.items():\n",
    "            print(f\"  - {key}: {value}\")\n",
    "    else:\n",
    "        print(f\"‚ùå Error: {result.stderr}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "\n",
    "print(\"\\n‚úÖ Monitoreo completado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# 3.2 Implementar recuperaci√≥n\n",
    "print(\"üîÑ IMPLEMENTANDO RECUPERACI√ìN\")\n",
    "print(\"=========================\")\n",
    "\n",
    "try:\n",
    "    # Verificar integridad\n",
    "    result = subprocess.run(['hdfs', 'fsck', '/', '-files', '-blocks', '-locations'], \n",
    "                          capture_output=True, text=True)\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        print(\"‚úÖ Verificaci√≥n de integridad completada\")\n",
    "        \n",
    "        # Crear punto de recuperaci√≥n\n",
    "        backup_dir = f'{analysis_dir}/backup_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}'\n",
    "        hdfs_client.makedirs(backup_dir)\n",
    "        \n",
    "        # Copiar archivos importantes\n",
    "        hdfs_client.copy(hdfs_path, f'{backup_dir}/test_large.txt')\n",
    "        hdfs_client.copy(compressed_path, f'{backup_dir}/test_compressed.gz')\n",
    "        \n",
    "        print(f\"‚úÖ Punto de recuperaci√≥n creado en {backup_dir}\")\n",
    "    else:\n",
    "        print(f\"‚ùå Error: {result.stderr}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "\n",
    "print(\"\\n‚úÖ Recuperaci√≥n implementada\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìù LIMPIEZA FINAL\n",
    "\n",
    "Limpiaremos los recursos utilizados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Limpiar recursos\n",
    "print(\"üßπ LIMPIANDO RECURSOS\")\n",
    "print(\"===================\")\n",
    "\n",
    "try:\n",
    "    # Eliminar archivos locales\n",
    "    if os.path.exists(test_file):\n",
    "        os.remove(test_file)\n",
    "        print(f\"‚úÖ Archivo local {test_file} eliminado\")\n",
    "    \n",
    "    # Eliminar directorio de an√°lisis\n",
    "    hdfs_client.delete(analysis_dir, recursive=True)\n",
    "    print(f\"‚úÖ Directorio {analysis_dir} eliminado de HDFS\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "\n",
    "print(\"\\n‚úÖ Limpieza completada\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
} 