#  Ejercicios Pr谩cticos HDFS

##  Ejercicios de Clase (Obligatorios)

### Ejercicio 1: Exploraci贸n B谩sica
1. Lista el contenido del directorio ra铆z de HDFS
2. Crea un directorio llamado `/datos` en HDFS
3. Crea subdirectorios para organizar datos:
   - `/datos/raw` para datos sin procesar
   - `/datos/processed` para datos procesados

### Ejercicio 2: Comandos Esenciales
1. Usa `hdfs dfs -ls` para listar archivos
2. Usa `hdfs dfs -cat` para ver el contenido de un archivo
3. Usa `hdfs dfs -df` para ver el espacio disponible

##  Ejercicios Adicionales (Opcionales)

### Ejercicio 3: Gesti贸n de Archivos
1. Sube un archivo de texto local a HDFS
2. Verifica que el archivo se haya subido correctamente
3. Crea una copia del archivo en el directorio de backup
4. Elimina el archivo original y verifica que la copia persista

### Ejercicio 4: An谩lisis de Bloques
1. Sube un archivo grande (>128MB) a HDFS
2. Usa `hdfs fsck` para ver c贸mo se dividi贸 en bloques
3. Verifica la replicaci贸n de los bloques
4. Analiza la distribuci贸n de los bloques entre DataNodes

### Ejercicio 5: Gesti贸n de Permisos
1. Crea un directorio `/datos/privado`
2. Establece permisos restrictivos (700)
3. Intenta acceder al directorio con diferentes usuarios
4. Modifica los permisos para permitir acceso de grupo

### Ejercicio 6: Monitoreo del Cluster
1. Usa `hdfs dfsadmin -report` para ver el estado del cluster
2. Identifica:
   - N煤mero de DataNodes activos
   - Espacio total disponible
   - Espacio usado
   - Estado de replicaci贸n

##  Ejercicios Avanzados (Opcionales)

### Ejercicio 7: Optimizaci贸n de Almacenamiento
1. Sube un conjunto de archivos peque帽os
2. Analiza el impacto en el espacio usado
3. Combina los archivos peque帽os en uno m谩s grande
4. Compara el espacio usado antes y despu茅s

### Ejercicio 8: Recuperaci贸n de Datos
1. Simula la ca铆da de un DataNode
2. Verifica c贸mo HDFS maneja la replicaci贸n
3. Recupera el DataNode
4. Analiza el proceso de rebalanceo

### Ejercicio 9: An谩lisis de Rendimiento
1. Mide el tiempo de escritura de archivos grandes
2. Mide el tiempo de lectura de archivos grandes
3. Analiza el throughput del cluster
4. Identifica cuellos de botella

##  Proyecto Final (Opcional)

### Ejercicio 10: Pipeline de Datos
1. Crea una estructura de directorios para un proyecto:
   ```
   /proyecto/
    raw/           # Datos sin procesar
    processed/     # Datos procesados
    backup/        # Copias de seguridad
    logs/          # Logs del sistema
   ```

2. Implementa un proceso de:
   - Subida de datos
   - Procesamiento
   - Backup
   - Limpieza

##  Notas Importantes

### Ejercicios de Clase
- Se desarrollar谩n durante la sesi贸n
- Son fundamentales para entender HDFS
- Se recomienda completarlos todos

### Ejercicios Adicionales
- Son opcionales pero recomendados
- Ayudan a profundizar en el conocimiento
- Pueden realizarse en cualquier momento

### Ejercicios Avanzados
- Son completamente opcionales
- Requieren m谩s tiempo y dedicaci贸n
- Ideales para pr谩ctica adicional

##  Recursos de Ayuda
- Documentaci贸n oficial de HDFS
- Comandos b谩sicos en el README
- Ejemplos en los notebooks de la sesi贸n
- Foros y comunidades de Hadoop

##  Criterios de Evaluaci贸n

### Nivel B谩sico
- Correcta ejecuci贸n de comandos b谩sicos
- Comprensi贸n de la estructura de directorios
- Manejo adecuado de archivos

### Nivel Intermedio
- An谩lisis correcto de bloques y replicaci贸n
- Gesti贸n efectiva de permisos
- Monitoreo b谩sico del cluster

### Nivel Avanzado
- Optimizaci贸n de almacenamiento
- Manejo de fallos y recuperaci贸n
- An谩lisis de rendimiento

### Ejercicio Final
- Implementaci贸n completa del pipeline
- Documentaci贸n detallada
- Soluci贸n de problemas
- Optimizaci贸n del sistema

##  Consejos
- Mant茅n un registro de todos los comandos utilizados
- Documenta cualquier error encontrado
- Prueba diferentes configuraciones
- Analiza el impacto de cada operaci贸n
- Verifica regularmente el estado del cluster 